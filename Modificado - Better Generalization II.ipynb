{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson #03_04 Task #02 Better Generalization II.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "myDHjbAjL-g3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j_KXUOdCGMe"
      },
      "source": [
        "# 1 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJH-P4J5CePE"
      },
      "source": [
        "In this lesson, you will continue to discover techniques to **improve the optimization** problem of adapting neural network model weights to learn a training dataset. Furthermore, it will be presented techniques that you can use to reduce overfitting and **improve the generalization** of your deep learning neural network models. After complete this study, you will know:\n",
        "\n",
        "- Improve the generalization\n",
        "  - How techniques that reduce model complexity have a **regularizing** effect resulting in less overtting and better generalization.\n",
        "  - How to add a **penalty to the loss function** to encourage **smaller model weights (L2)**.\n",
        "  - How to add a **penalty to the loss function** to encourage **sparse internal representations (L1)**.\n",
        "  - How to add a **constraint to the model** to force **small model weights** and lower complexity models.\n",
        "  - How to add **dropout** weights during training to decouple model layers.\n",
        "  - How to **add noise** to the training process to promote model robustness.\n",
        "  - How to use **early stopping** to halt model training at the right time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myDHjbAjL-g3"
      },
      "source": [
        "# 2 Better Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqGNfmJaQNgR"
      },
      "source": [
        "## 2.1 Fix overfitting with regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxgfftXLMIUW"
      },
      "source": [
        "Training a deep neural network that can generalize well to new data is a challenging problem. A model with too little capacity cannot learn the problem, whereas a model with too much capacity can learn it too well and overfit the training dataset. Both cases result in a model that does not generalize well. \n",
        "\n",
        "> A modern approach to reducing generalization error is to use a larger model that may be required to use **regularization during training** that keeps the model's weights small. \n",
        "\n",
        "These techniques reduce overfitting and lead to faster optimization of the model and better overall performance. This section will discover the problem of overfitting when training neural networks and how it can be addressed with regularization methods. After reading this section, you will know:\n",
        "\n",
        "- **Underfitting can quickly** be addressed by increasing the network's capacity, but overfitting requires the use of specialized techniques.\n",
        "- Regularization methods like **weight decay** provide an easy way to **control overfitting** for sizeable neural network models.\n",
        "- A modern recommendation for regularization is to use **early stopping** with **dropout** and a **weight constraint**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9BCJtoDPcEE"
      },
      "source": [
        "### 2.1.1 Problem of model generalization and overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZMCZ4S9QnD4"
      },
      "source": [
        "A neural network's objective is to have a final model that performs well on the data that we used to train it (e.g., the training dataset) and the new data on which the model will be used to make predictions.\n",
        "\n",
        "> The central challenge in machine learning is that we must perform well on new, previously unseen inputs - not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called **generalization**.\n",
        "\n",
        "We require that the model learn from known examples and generalize them to new examples in the future. We use methods like a train/test split or k-fold cross-validation to estimate the model's ability to generalize to new data. **learning and also generalizing to new cases is challenging**. \n",
        "\n",
        "Too little learning and the model will perform poorly on\n",
        "the training dataset and on new data. **The model will underfit the problem**. \n",
        "\n",
        "Too much learning and the model will perform well on the training dataset and poorly on new data, the model will **overfit the problem**. In both cases, the model has not been generalized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n_1Exr1RAMr"
      },
      "source": [
        "A model fit can be considered in the context of the <font color=\"red\">bias-variance trade-off</font>. \n",
        "\n",
        "> An underfit model has high bias and low variance. \n",
        "\n",
        "Regardless of the specific samples in the training data, it cannot learn the problem. \n",
        "> An overfit model has low bias and high variance. \n",
        "\n",
        "The model learns the training data too well and performance varies widely with new unseen examples or even statistical noise added to examples in the training dataset.\n",
        "\n",
        "**We can address underfitting by increasing the capacity of the model**. \n",
        "\n",
        "> Capacity refers to a model's ability to fit a variety of functions; more capacity means that a model can fit more types of functions for mapping inputs to outputs. \n",
        "\n",
        "Increasing the capacity of a model is\n",
        "easily achieved by changing the structure of the model, such as adding more layers and/or more nodes to layers. Because an underfit model is so easily addressed, it is more common to have an\n",
        "overfit model. \n",
        "\n",
        "**An overfit model is easily diagnosed by monitoring the model's performance during training** by evaluating both a training dataset and a holdout validation dataset. Graphing line plots of the performance of the model during training, called learning curves, will show a familiar pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu7A6rjJUFR7"
      },
      "source": [
        "### 2.1.2 Reduce Overfitting by Constraining Complexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuwOTt1VjD4"
      },
      "source": [
        "There are two ways to approach an overfit model:\n",
        "\n",
        "1. Reduce overfitting by training the network on more examples.\n",
        "2. Reduce overfitting by changing the complexity of the network.\n",
        "\n",
        "A benefit of very deep neural networks is that their performance improves as they are fed larger and larger datasets. A model with a near-infinite number of examples will eventually plateau in terms of the network's capacity is capable of learning. **A model can overfit a training dataset because it has sufficient capacity to do so**. Reducing the model's capacity reduces the likelihood of the model overfitting the training dataset to a point where it no longer overfits. The capacity of a neural network model, its complexity, is defined by its structure in terms of nodes and layers and the parameters in terms of its weights. Therefore, **we can reduce the complexity of a neural network to reduce overfitting in one of two\n",
        "ways**:\n",
        "\n",
        "1. Change network complexity by changing the network structure (number of weights).\n",
        "2. Change network complexity by changing the network parameters (values of weights)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywdlx84PWcNn"
      },
      "source": [
        "For example, the structure could be tuned via grid search until a suitable number of nodes and/or layers are found to reduce or remove overfitting for the problem. Alternately, the model could be overfitted and pruned by removing nodes until it achieves suitable performance on\n",
        "a validation dataset. It is more common to instead constrain the model's complexity by **ensuring the parameters (weights) of the model remain small**. Small parameters suggest a less complex and, in turn, more stable model that is less sensitive to statistical fluctuations in the\n",
        "input data.\n",
        "\n",
        "> **Regularization** is any modification we make to a learning algorithm that is intended to **reduce its generalization error** but not its training error. Regularization is one of the central concerns of the \ffield of machine learning, rivaled in its importance only by optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09noDgveYnBn"
      },
      "source": [
        "### 2.1.3 Regularization Methods for Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SKjJYg_u4ka"
      },
      "source": [
        "The simplest and perhaps **most common regularization method** is to **add a penalty to the loss function** in proportion to the weights' size in the model.\n",
        "\n",
        "\n",
        "<font color=\"red\">Weight Regularization:</font> Penalize the model during training **based on the magnitude of the weights**.\n",
        "\n",
        "> This will encourage the model to map the inputs to the outputs of the training dataset so that the weights of the model are kept small. This approach is called weight regularization of **weight decay** and has proven very effective for decades for both more straightforward linear\n",
        "models and neural networks.\n",
        "\n",
        "Below is a list of \ffive of the most common additional regularization methods.\n",
        "\n",
        "- **Activity Regularization**: Penalize the model during training based on the magnitude\n",
        "of the activations.\n",
        "- **Weight Constraint**: Constrain the magnitude of weights to be within a range or below a limit.\n",
        "- **Dropout**: Probabilistically remove inputs during training.\n",
        "- **Noise**: Add statistical noise to inputs during training.\n",
        "- **Early Stopping**: Monitor model performance on a validation set and stop training when performance degrades.\n",
        "\n",
        "Some more specific recommendations include:\n",
        "\n",
        "- **Classical**: use early stopping and weight decay (L2 weight regularization).\n",
        "- **Alternate**: use early stopping and added noise with a weight constraint.\n",
        "- **Modern**: use early stopping and dropout, in addition to a weight constraint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiLxRt2kA3a4"
      },
      "source": [
        "## 2.2 Penalize Large Weights with Weight Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6wdExEGoHCL"
      },
      "source": [
        "Neural networks learn a set of weights that best map inputs to outputs. **A network with large network weights can sign an unstable network** where small changes in the input can lead to large changes in the output. This can signify that the network has overfitted the training dataset and will likely perform poorly when making predictions on new data. \n",
        "\n",
        "A solution to this problem is to update the learning algorithm to encourage the network to keep the weights small. This is called **weight regularization**, and it can be used as a general technique to reduce\n",
        "the overfitting of the training dataset and improve the model's generalization. In this section, you will discover weight regularization as an approach to reduce overfitting for neural networks. After reading this tutorial, you will know:\n",
        "\n",
        "- Large weights in a neural network signify a more complex network that has overfit the training data.\n",
        "- Penalizing a network based on the size of the network weights during training can reduce overfitting.\n",
        "- An L1 or L2 vector norm penalty can be added to the network's optimization to encourage smaller weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0gLxANIqdc0"
      },
      "source": [
        "### 2.2.1 Weight Regularization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7m2JsXFrFU0"
      },
      "source": [
        "When fitting a neural network model, we must learn the network's weights (i.e., the model parameters) using stochastic gradient descent and the training dataset. The longer we train the network, the more specialized the weights will become to the training data, overfitting the training data. The weights will grow in size to handle the specifics of the examples seen\n",
        "in the training data. **Large weights make the unstable network**. Although the weights will be specialized to the training dataset, minor variation or statistical noise on the expected inputs will result in large differences in the output.\n",
        "\n",
        "Generally, **we refer to this model as having a large variance and a small bias**. That is, the model is sensitive to the specific examples, the statistical noise, in the training dataset. \n",
        "\n",
        "> A model with large weights is more complex than a model with smaller weights. \n",
        "\n",
        "It is a sign of a network that may be overly specialized to training data. In practice, we prefer to choose the simpler\n",
        "models to solve a problem. **We prefer models with smaller weights**.\n",
        "\n",
        "Another possible issue is that there may be many input variables, each with different relevance levels to the output variable. Sometimes we can use methods to help select input variables, but the interrelationships between variables are often not obvious. **Having small weights\n",
        "or even zero weights for less relevant or irrelevant inputs to the network will allow the model to focus on learning**. This, too, will result in a simpler model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g4sj4WBsSJ7"
      },
      "source": [
        "### 2.2.2 Encourage Small Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYpU1Cmquwee"
      },
      "source": [
        "The learning algorithm can be updated to encourage the network toward using small weights. One way to do this is to change the calculation of loss used in the network's optimization to consider the weights' size. Remember that we minimize a loss function when we train a neural network, such as the log loss in classification or mean squared error in regression. **We can add the current size of all weights in the network** or add a layer to this calculation in calculating the loss between the predicted and expected values in a batch. **This is called a penalty because we are penalizing the model proportional to the size of the weights in the model.**\n",
        "\n",
        "> Many regularization approaches are based on limiting the capacity of models, such\n",
        "as neural networks, linear regression, or logistic regression, by adding a [...] penalty to the objective function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN_QsdMNvkJH"
      },
      "source": [
        "**Larger weights result in a larger penalty in the form of a larger loss score.** The optimization algorithm will then push the model to have smaller weights, i.e., weights no larger than needed to perform well on the training dataset. \n",
        "\n",
        "**Smaller weights are considered more regular or less specialized**, and as such, we refer to this penalty as **weight regularization**. When this approach of penalizing model coefficients is used in other machine learning models such as linear regression or logistic regression, it may be referred to as **shrinkage** because the penalty encourages the\n",
        "coefficients shrink during the optimization process.\n",
        "\n",
        "> The addition of a weight size penalty or weight regularization to a neural network has the effect of reducing generalization error and of allowing the model to pay less attention to less relevant input variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2c3NsP_9tQZ"
      },
      "source": [
        "### 2.2.3 How to Penalize Large Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cCNbFwNCd_L"
      },
      "source": [
        "There are two parts to penalizing the model based on the size of the weights. The first is the calculation of the **size of the weights**, and the second is **the amount of attention** that the optimization process should pay to the penalty.\n",
        "\n",
        "**Calculate Weight Size**\n",
        "\n",
        "Neural network weights are real-values that can be positive or negative, as such, simply adding the weights is not sufficient. There are two main approaches used to calculate the size of the weights, they are:\n",
        "\n",
        "- Calculate the sum of the absolute values of the weights, called the L1 norm (or $L^1$).\n",
        "- Calculate the sum of the squared values of the weights, called the L2 norm (or $L^2$).\n",
        "\n",
        "> L1 encourages weights to 0.0 if possible, resulting in more sparse weights (weights with more 0.0 values). \n",
        "\n",
        "> L2 offers more nuance, both penalizing larger weights more severely but resulting in less sparse weights. \n",
        "\n",
        "The use of L2 in linear and logistic regression is often referred to as **Ridge Regression**. This is useful when trying to develop an intuition for the penalty or examples of its usage.\n",
        "\n",
        "**Control Impact of the Penalty**\n",
        "\n",
        "The calculated size of the weights is added to the loss objective function when training the network. Rather than adding each weight to the penalty directly, they can be weighted using a new hyperparameter called alpha ($\\alpha$) or sometimes lambda. \n",
        "\n",
        "This controls the amount of attention that the learning process should pay to the penalty. Put another way, the amount to penalize the model based on the size of the weights. \n",
        "\n",
        "> The $\\alpha$ hyperparameter has a value between 0.0 (no penalty) and 1.0 (full penalty). This hyperparameter controls the model's amount of bias from 0.0, or low bias (high variance), to 1.0, or high bias (low variance).\n",
        "\n",
        "- If the penalty is too strong, the model will underestimate the weights and underfit the problem. \n",
        "- If the penalty is too weak, the model will be allowed to overfit the training data. \n",
        "\n",
        "The vector norm of the weights is often calculated per-layer rather than across the entire network.\n",
        "\n",
        "This allows more flexibility in the choice of the type of regularization used (e.g., L1 for inputs, L2 elsewhere) and flexibility in the $\\alpha$ value, although **it is common to use the same $\\alpha$\n",
        "value on each layer by default.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0IXgMkMDjfl"
      },
      "source": [
        "### 2.2.4 Weight Regularization Case Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qkvQsvzFr02"
      },
      "source": [
        "This section will **demonstrate how to use weight regularization** to reduce the overfitting of an MLP on a simple binary classification problem. This example provides a template for applying weight regularization to your neural network for classification and regression problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxqFFjBDlI9U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "9ab10c8d-35a5-4621-e327-53ef4b338d35"
      },
      "source": [
        "# scatter plot of moons dataset\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# scatter plot for each class value\n",
        "for class_value in range(2):\n",
        "\t# select indices of points with the class label\n",
        "\trow_ix = np.where(y == class_value)\n",
        "\t# scatter plot for points with a different color\n",
        "\tplt.scatter(x[row_ix, 0], x[row_ix, 1])\n",
        "\n",
        "# show plot\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P3caslF-NaQ"
      },
      "source": [
        "#### 2.2.4.1 Overfit Multilayer Perceptron Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgnLP85r-uRz"
      },
      "source": [
        "We can develop an MLP model to address this binary classification problem. The model will have one hidden layer with more nodes required to solve this problem, providing an opportunity to overfit. We will also train the model for longer than is necessary to ensure the model overfits. Before defining the model, we will split the dataset into train and test sets, using 30 examples to train the model and 70 to evaluate the t model's performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOw0_X0HSlqP"
      },
      "source": [
        "import tensorflow as tf\n",
        "early = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=200, verbose=1,\n",
        "    mode='auto', restore_best_weights=True\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goAvXMI5_NAu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "b9ea0db9-2745-4ed6-dd42-70ae5819209a"
      },
      "source": [
        "# overfit mlp for the moons dataset\n",
        "from sklearn.datasets import make_moons\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# split into train and test sets\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y), \n",
        "                    epochs=4000, verbose=0,batch_size=32,\n",
        "                    callbacks=[early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 01171: early stopping\n",
            "Train: 1.000, Test: 0.929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSVkM1FPAba5"
      },
      "source": [
        "Line plots showing learning curves of cross-entropy loss and classification accuracy on the train and test sets for each training epoch are also created. **The learning curve for loss shows a clear overfitting pattern**, mirrored in the learning curve for the classification accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL4UaPfgB-6L"
      },
      "source": [
        "#### 2.2.4.2 MLP Model With Weight Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51wxbEf8CEXw"
      },
      "source": [
        "# mlp with weight regularization for the moons dataset\n",
        "from sklearn.datasets import make_moons\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# split into train and test sets\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu',\n",
        "                kernel_regularizer=l2(0.001)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y, \n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWzPS5HwC5V0"
      },
      "source": [
        "We would expect that it would have also changed the telltale learning curve for overfitting through weight regularization. Instead of improving the model's accuracy on the test set increasing and then decreasing again, we should see it continually rise during training. As expected, we see the learning curves for loss and accuracy on the test dataset plateau, indicating that the model has no longer overt the training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUhIZfm9H-dA"
      },
      "source": [
        "#### 2.2.4.3 Grid Search Regularization Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd-uLFuGIHbo"
      },
      "source": [
        "# grid search regularization values for moons dataset\n",
        "from sklearn.datasets import make_moons\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# grid search values\n",
        "values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
        "all_train, all_test = list(), list()\n",
        "for param in values:\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(500, input_dim=2, activation='relu', kernel_regularizer=l2(param)))\n",
        "  model.add(Dropout(0.1))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t\n",
        "  # fit model\n",
        "\tmodel.fit(train_x, train_y, epochs=4000, verbose=0, callbacks=[early])\n",
        " \n",
        "\t# evaluate the model\n",
        "\t_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "\t_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "\tprint('Param: %f, Train: %.3f, Test: %.3f' % (param, train_acc, test_acc))\n",
        "\tall_train.append(train_acc)\n",
        "\tall_test.append(test_acc)\n",
        " \n",
        "# plot train and test means\n",
        "plt.semilogx(values, all_train, label='train', marker='o')\n",
        "plt.semilogx(values, all_test, label='test', marker='o')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsnklaweK3l2"
      },
      "source": [
        "A line plot of the results is also created, showing the increase in test accuracy with larger weight regularization parameter values, at least to a point. We can see that using the largest value of 0.1 results in a large drop in both train and test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al_KOvFXMQVD"
      },
      "source": [
        "## 2.3 Force Small Weights with Weight Constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdY7SRadMUvD"
      },
      "source": [
        "Weight regularization methods like weight decay introduce a penalty to the loss function when training a neural network to encourage the network to use small weights. Smaller weights in a neural network can result in a model that is more stable and less likely to overfit the training\n",
        "dataset, in turn, have better performance when making a prediction on new data. \n",
        "\n",
        "> Unlike weight regularization, a weight constraint is a trigger that checks the size or magnitude of the weights and scales them so that they are all below a pre-defined threshold. \n",
        "\n",
        "The constraint forces weights to be small and can be used instead of weight decay and in conjunction with more aggressive network configurations, such as very large learning rates. **In this section, you\n",
        "will discover the use of weight constraint regularization as an alternative to weight penalties to reduce overfitting in deep neural networks**. After reading this section, you will know:\n",
        "\n",
        "- Weight penalties encourage but do not require neural networks to have small weights.\n",
        "- Weight constraints, such as the L2 norm and maximum norm, can be used to force neural networks to have small weights during training.\n",
        "- Weight constraints can improve generalization when used in conjunction with other regularization methods like a dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgbX6JEONhax"
      },
      "source": [
        "### 2.3.1 Weight Constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da59TDvvQhUL"
      },
      "source": [
        "In this section you will discover the problem with neural networks that have large weighs, a technique that you can use to force the development of models with small weights called **weight\n",
        "constraints** and tips for using this technique in your own projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJV5ijosQtru"
      },
      "source": [
        "#### 2.3.1.1 Alternative to penalties for large weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd8Kcjl7Q7I_"
      },
      "source": [
        "**Large weights in a neural network are a sign of overfitting**. A network with large weights has very likely learned the statistical noise in the training data. This results in a model that is unstable and very sensitive to changes to the input variables. \n",
        "\n",
        "> In turn, the overfit network\n",
        "has poor performance when making predictions on new unseen data. \n",
        "\n",
        "A popular and effective technique to address the problem is to update the loss function that is optimized during training to take the size of the weights into account.\n",
        "\n",
        "This is called a penalty, as the larger the weights of the network become, the more the network is penalized, resulting in larger loss and, in turn, larger updates. The effect is that the penalty encourages weights to be small or no larger than is required during the training process, in turn reducing overfitting. \n",
        "\n",
        "> A problem in using a penalty is that although it does encourage the network toward smaller weights, it does not force smaller weights. **A neural network trained with weight regularization penalty may still allow large weights, in some cases very large weights.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEyokKmHRTpa"
      },
      "source": [
        "#### 2.3.1.2 Force Small Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP1S-KWQSyKv"
      },
      "source": [
        "**An alternate solution to using a penalty for the size of network weights is to use a weight constraint**. A weight constraint is an update to the network that checks the size of the weights (e.g., their vector norm), and if the size exceeds a predefined limit, the weights are rescaled so\n",
        "that their size is below the limit or between a range. You can think of a weight constraint as an if-then rule checking the size of the weights while the network is being trained and only coming into effect and making weights small when required. Note, for efficiency; it does not have to be\n",
        "implemented as an if-then rule and often is not.\n",
        "\n",
        "Unlike adding a penalty to the loss function, a weight constraint ensures the weights of the network are small instead of merely encouraging them to be small. It can be useful on those problems or with networks that resist other regularization methods, such as weight penalties. Weight constraints prove especially useful when you have configured your network to use alternative regularization methods to weight regularization and yet still desire the network to have small weights in order to reduce overfitting. \n",
        "\n",
        "> One often-cited example is the use of a **weight constraint regularization with dropout regularization**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioas878TTHME"
      },
      "source": [
        "#### 2.3.1.3 How to use a weight constraint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwFnIKVaT-bA"
      },
      "source": [
        "A constraint is enforced on each node within a layer. All nodes within the layer use the same constraint, and often multiple hidden layers within the same network will use the same constraint.\n",
        "\n",
        "Recall that when we talk about the vector norm in general, that this is the magnitude of the vector of weights in a node, and by default, is calculated as the L2 norm, e.g., the square root of the sum of the squared values in the vector. Some examples of constraints that could be used\n",
        "include:\n",
        "\n",
        "- Force the vector norm to be 1.0 (e.g., the unit norm).\n",
        "- Limit the maximum size of the vector norm (e.g., the maximum norm).\n",
        "- Limit the minimum and maximum size of the vector norm (e.g., the min-max norm).\n",
        "\n",
        "The **maximum norm**, also called max-norm or maxnorm, is a popular constraint because it is less aggressive than other norms such as the unit norm, simply setting an upper bound.\n",
        "\n",
        "When **using a limit or a range**, a hyperparameter must be specified. Given that weights are small, the hyperparameter, too, is often a small integer value, such as a value between 1 and 4.\n",
        "\n",
        "If the **norm exceeds the specified range or limit**, the weights are rescaled or normalized such that their magnitude is below the specified parameter or within the specified range.\n",
        "\n",
        "> The constraint can be applied after each update to the weights, e.g., at the end of each minibatch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5ra7V6tW4rB"
      },
      "source": [
        "The Keras API supports weight constraints. The constraints are specified per-layer but applied and enforced per-node within the layer. Using a constraint generally involves setting the **kernel\\_constraint** argument on the layer for the input weights and the **bias\\_constraint** for the bias weights. **Generally, weight constraints are not used on the bias weights**. A suite of different vector norms can be used as constraints, provided as classes in the **keras.constraints** module. They are:\n",
        "\n",
        "- **Maximum norm** (max\\_norm), to force weights to have a magnitude at or below a given limit.\n",
        "- **Non-negative norm** (non\\_neg), to force weights to have a positive magnitude.\n",
        "- **Unit norm** (unit\\_norm), to force weights to have a magnitude of 1.0.\n",
        "- **Min-Max norm** (min\\_max\\_norm), to force weights to have a magnitude between a range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fsTWvrGawQr"
      },
      "source": [
        "### 2.3.2 Weight Constraints Case Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdpvDriibXwh"
      },
      "source": [
        "In this section, **we will demonstrate how to use weight constraints to reduce overfitting of an MLP on a simple binary classification problem**. This example provides a template for applying weight constraints to your own neural network for classification and regression problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnB0LV-wbioW"
      },
      "source": [
        "# scatter plot of moons dataset\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# scatter plot for each class value\n",
        "for class_value in range(2):\n",
        "\t# select indices of points with the class label\n",
        "\trow_ix = np.where(y == class_value)\n",
        "\t# scatter plot for points with a different color\n",
        "\tplt.scatter(x[row_ix, 0], x[row_ix, 1])\n",
        "# show plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v9mDw97by79"
      },
      "source": [
        "#### 2.3.2.1 Overfit multilayer perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AqIuw53b8z2"
      },
      "source": [
        "# mlp overfit on the moons dataset\n",
        "from sklearn.datasets import make_moons\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0,callbacks=[early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TyVNsxXcj1o"
      },
      "source": [
        "Running the example reports the model performance on the train and test datasets. We can see that the model has better performance on the training dataset than the test dataset, one possible sign of overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN7mQcAEdQdD"
      },
      "source": [
        "#### 2.3.2.2 Overfit MLP With Weight Constraint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zClD-Fcrden4"
      },
      "source": [
        "We can update the example to use a weight constraint. There are a few different weight\n",
        "constraints to choose from. A good simple constraint for this model is to simply normalize the weights so that the norm is equal to 1.0. \n",
        "\n",
        "This constraint has the effect of forcing all incoming weights to be small. We can do this by using the unit norm in Keras. This constraint can be added to the \ffirst hidden layer as follows:\n",
        "\n",
        "```python\n",
        "model.add(Dense(500, input_dim=2, activation='relu', kernel_constraint=unit_norm()))\n",
        "```\n",
        "\n",
        "We can also achieve the same result by using the min max norm and setting the min and maximum to 1.0, for example:\n",
        "\n",
        "```python\n",
        "model.add(Dense(500, input_dim=2, activation='relu',kernel_constraint=min_max_norm(min_value=1.0, max_value=1.0)))\n",
        "```\n",
        "\n",
        "We cannot achieve the same result with the maximum norm constraint as it will allow norms at or below the speci\fed limit; for example:\n",
        "\n",
        "```python\n",
        "model.add(Dense(500, input_dim=2, activation='relu',kernel_constraint=max_norm(1.0)))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCtjLrjPd2QS"
      },
      "source": [
        "# mlp overfit on the moons dataset with a unit norm constraint\n",
        "from sklearn.datasets import make_moons\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.constraints import unit_norm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu', kernel_constraint=unit_norm()))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xCi14I_e2cc"
      },
      "source": [
        "Reviewing the line plot of train and test loss and accuracy, we can see that it no longer appears that the model has overfit the training dataset. Model accuracy on both the train and test sets continues to improve to a plateau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iITgC-qTftXY"
      },
      "source": [
        "## 2.4 Decouple Layers with Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUNhTPblghgN"
      },
      "source": [
        "Deep learning neural networks are likely to quickly overfit a training dataset with few examples. Ensembles of neural networks with different model configurations are known to reduce overfitting but require the additional computational expense of training and maintaining multiple models. \n",
        "\n",
        "> A single model can be used to simulate having a large number of different network architectures by randomly dropping out nodes during training. \n",
        "\n",
        "This is called **dropout** and **offers a very computationally cheap and remarkably effective regularization method to reduce overfitting and\n",
        "generalization error in deep neural networks of all kinds**. \n",
        "\n",
        "In this section, you will discover the use of dropout regularization for reducing overfitting and improving the generalization of deep neural networks. After reading this section, you will know:\n",
        "\n",
        "- Large weights in a neural network are a sign of a more complex network that has overfitted the training data.\n",
        "- Probabilistically dropping out nodes in the network is a simple and effective regularization method.\n",
        "- A large network with more training epochs and the use of a weight constraint is suggested when using dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC_bV5hYj8Rf"
      },
      "source": [
        "### 2.4.1 Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOqqqktckatd"
      },
      "source": [
        "In this section you will discover that you can simulate the development of a large ensemble of neural network models in a single model called **dropout**, how you can use it to reduce overfitting, and tips for using this technique on your own projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V33XI1ZVkkYc"
      },
      "source": [
        "#### 2.4.1.1 Problem With Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi1qzSrOkp5u"
      },
      "source": [
        "Large neural nets trained on relatively small datasets can overfit the training data. This has the effect of the model learning the statistical noise in the training data, which results in poor performance when the model is evaluated on new data, e.g., a test dataset. Generalization error\n",
        "increases due to overfitting. One approach to reducing overfitting is to fit all possible different neural networks on the same dataset and to average the predictions from each model. This is not feasible in practice, and can be approximated using a small collection of different models,\n",
        "called an ensemble.\n",
        "\n",
        "> A problem even with the ensemble approximation is that it requires multiple models to be fit and stored, which can be a challenge if the models are large, requiring days or weeks to train and tune."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFytUU-2lQsk"
      },
      "source": [
        "#### 2.4.1.2 Randomly Drop Nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQb-AEpwldLm"
      },
      "source": [
        "**Dropout is a regularization method** that approximates training a large number of neural networks with different architectures in parallel. \n",
        "\n",
        "> During training, some number of node outputs are randomly ignored or dropped out. \n",
        "\n",
        "This has the effect of making the layer look-like and be\n",
        "treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different view of the configured layer.\n",
        "\n",
        "> **Dropout has the effect of making the training process noisy**, forcing nodes within a layer to probabilistically take on more or less responsible for the inputs.\n",
        "\n",
        "This conceptualization suggests that perhaps dropout breaks-up situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust.\n",
        "\n",
        "Because the outputs of a layer under dropout are randomly subsampled, it has the effect of reducing the capacity or thinning the network during training. As such, a wider network, e.g., more nodes, may be required when using dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyDPIx24mYlP"
      },
      "source": [
        "#### 2.4.1.3 How to Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x0Mzg-Hm20G"
      },
      "source": [
        "Dropout is implemented per-layer in a neural network. It can be used with most types of layers, such as dense fully connected layers, convolutional layers, and recurrent layers such as the long\n",
        "short-term memory network layer. \n",
        "\n",
        "> Dropout may be implemented on any or all hidden layers in the network as well as the visible or input layer. **It is not used on the output layer**.\n",
        "\n",
        "\n",
        "**A new hyperparameter** is introduced that specifies the probability at which outputs of the layer are dropped out, or inversely, the probability at which outputs of the layer are retained.\n",
        "\n",
        "The interpretation is an implementation detail that can differ from paper to code library. \n",
        "\n",
        "> A common value is a probability of 0.5 for retaining the output of each node in a hidden layer and a value close to 1.0, such as 0.8, for retaining inputs from the visible layer.\n",
        "\n",
        "**Dropout is not used after training when making a prediction with the \ffit network**. The weights of the network will be larger than normal because of dropout. Therefore, before \ffinnalizing\n",
        "the network, the weights are \ffirrst scaled by the chosen dropout rate. The network can then be used as per normal to make predictions.\n",
        "\n",
        "The rescaling of the weights can be performed at training time instead, after each weight update at the end of the minibatch. \n",
        "\n",
        "> This is sometimes called **inverse dropout** and does not\n",
        "require any modification of weights during training. Both the Keras and PyTorch deep learning libraries implement dropout in this way.\n",
        "\n",
        "**Dropout works well in practice**, perhaps replacing the need for weight regularization (e.g. weight decay) and activation regularization (e.g. representation sparsity)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBctII-znHgn"
      },
      "source": [
        "### 2.4.2 Dropout Case Study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG-62OzWqrs4"
      },
      "source": [
        "# scatter plot of circles dataset\n",
        "from sklearn.datasets import make_circles\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n",
        "\n",
        "# scatter plot for each class value\n",
        "for class_value in range(2):\n",
        "\t# select indices of points with the class label\n",
        "\trow_ix = np.where(y == class_value)\n",
        " \n",
        "\t# scatter plot for points with a different color\n",
        "\tplt.scatter(x[row_ix, 0], x[row_ix, 1])\n",
        "# show plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNCbJXx5q-ng"
      },
      "source": [
        "#### 2.4.2.1 Overfit multilayer perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl0lA9xgrHcW"
      },
      "source": [
        "# mlp overfit on the two circles dataset\n",
        "from sklearn.datasets import make_circles\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbdgc9vqrlp-"
      },
      "source": [
        "#### 2.4.2.2 MLP with dropout regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWQlHlmxsR4d"
      },
      "source": [
        "We can update the example to use dropout regularization. We can do this by simply inserting a new Dropout layer between the hidden layer and the output layer. In this case, we will specify a dropout rate (probability of setting outputs from the hidden layer to zero) to 40% or 0.4.\n",
        "\n",
        "```python\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln4kDU1Csb9b"
      },
      "source": [
        "# mlp with dropout on the two circles dataset\n",
        "from sklearn.datasets import make_circles\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxvzLw2EtAUr"
      },
      "source": [
        "In this specific case, we can see that dropout resulted in a slight drop in accuracy on the training dataset, down from 100% to 96%, and a lift in accuracy on the test set, up from 75% to 80%.\n",
        "\n",
        "Reviewing the line plot of train and test accuracy during training, we can see that it no longer appears that the model has overfit the training dataset. Model accuracy on both the train and test sets continues to increase to a plateau, albeit with a lot of noise given the use of dropout during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x32mjaCoupUk"
      },
      "source": [
        "## 2.5 Promote Robustness with Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9looEgERr5W"
      },
      "source": [
        "Training a neural network with a small dataset can cause the network to memorize all training examples, in turn leading to poor performance on a holdout dataset. Small datasets may also represent a harder mapping problem for neural networks to learn, given the patchy or sparse\n",
        "a sampling of points in the high-dimensional input space. \n",
        "\n",
        "> One approach to making the input space smoother and easier to learn is to add noise to inputs during training. \n",
        "\n",
        "In this section, **you will discover that adding noise to a neural network during training can improve the robustness of the network, resulting in better generalization and faster learning**. \n",
        "\n",
        "After reading this section, you will know:\n",
        "\n",
        "- Small datasets can make learning challenging for neural nets, and the examples can be memorized.\n",
        "- Adding noise during training can make the training process more robust and reduce generalization error.\n",
        "- Noise is traditionally added to the inputs but can also be added to weights, gradients, and even activation functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1eSzzBFStMO"
      },
      "source": [
        "### 2.5.1 Noise Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqXzpIJdS1fw"
      },
      "source": [
        "In this section, you will discover the brittleness of large network weights and how the addition of statistical noise can provide a regularizing effect, as well as tips to help when adding noise to your own neural network models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsDUsng-TGY7"
      },
      "source": [
        "#### 2.5.1.1 Add Random Noise During Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMW1nAOcTvqj"
      },
      "source": [
        "One approach to improving generalization error and to improving the structure of the mapping problem is to add random noise.\n",
        "\n",
        "At first, this sounds like a recipe for making learning more challenging. It is a counter-intuitive suggestion to improving performance because one would expect noise to degrade performance of the model during training.\n",
        "\n",
        "> Heuristically, we might expect that the noise will 'smear out' each data point and make it dicult for the network to fit individual data points precisely, and hence will reduce overfitting. In practice, it has been demonstrated that training with noise can indeed lead to improvements in network generalization.\n",
        "\n",
        "The addition of noise during the training of a neural network model has a regularization effect and, in turn, improves the robustness of the model. It has been shown to have a similar impact on the loss function as the addition of a penalty term, as in the case of weight regularization\n",
        "methods.\n",
        "\n",
        "In effect, adding noise expands the size of the training dataset. Each time a training sample is exposed to the model, random noise is added to the input variables making them different every time it is exposed to the model. In this way, adding noise to input samples is a simple\n",
        "form of **data augmentation**.\n",
        "\n",
        "> Injecting noise in the input to a neural network can also be seen as a form of data\n",
        "augmentation.\n",
        "\n",
        "**Adding noise means that the network is less able to memorize training samples because they are changing all of the time**, resulting in smaller network weights and a more robust network that has lower generalization error. The noise means that it is as though new samples are being drawn from the domain in the vicinity of known samples, smoothing the structure of the input space. This smoothing may mean that the mapping function is easier for the network to learn, resulting in better and faster learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBSGxkW0Utog"
      },
      "source": [
        "#### 2.5.1.2 How and Where to Add Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE5KERASZexV"
      },
      "source": [
        "The most common type of **noise used during training** is the addition of **Gaussian noise** to input variables. Gaussian noise, or white noise, has a **mean of zero and a standard deviation of one** and can be generated as needed using a pseudorandom number generator. The addition of Gaussian noise to the inputs to a neural network was traditionally referred to as **jitter or random jitter** after the use of the term in signal processing to refer to the uncorrelated random noise\n",
        "in electrical circuits. \n",
        "\n",
        "> The amount of noise added (e.g. the spread or standard deviation) is a configurable hyperparameter. \n",
        "\n",
        "**Too little noise has no effect, whereas too much noise makes the\n",
        "mapping function too challenging to learn.**\n",
        "\n",
        "The **standard deviation of the random noise** controls the amount of spread and can be adjusted based on the scale of each input variable. It can be easier to configure if the scale of the input variables has first been normalized. <font color='red'>Noise is only added during training</font>. No noise is added during the evaluation of the model or when the model is used to make predictions on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHF3jwbucJih"
      },
      "source": [
        "Although additional noise to the inputs is the most common and widely studied approach, random noise can be added to other network parts during training. Some examples include:\n",
        "\n",
        "- **Add noise to activations**, i.e., the outputs of each layer.\n",
        "- **Add noise to weights**, i.e., an alternative to the inputs.\n",
        "- **Add noise to the gradients**, i.e., the direction to update weights.\n",
        "- **Add noise to the outputs**, i.e., the labels or target variables.\n",
        "\n",
        "The **addition of noise** to the **layer activations** allows noise to be used at any point in the network. This can be **beneficial for very deep networks**. Noise can be added to the layer outputs themselves, but this is more likely achieved via a noisy activation function. \n",
        "\n",
        "The **addition of noise** to **weights** allows the approach to be used throughout the network consistently instead of adding noise to inputs and layer activations. This is **particularly useful in recurrent neural networks**.\n",
        "\n",
        "The **addition of noise** to **gradients** focuses more on **improving the optimization process's robustness** rather than the structure of the input domain. The amount of noise can start high at the beginning of training and decrease over time, much like a decaying learning rate.\n",
        "This approach has proven to be an effective method for very deep networks and a variety of different network types.\n",
        "\n",
        "> Adding noise to the activations, weights, or gradients all provides a more generic approach to adding noise invariant to the types of input variables provided to the model. \n",
        "\n",
        "If the problem domain is believed or expected to have mislabeled examples, then the addition of noise\n",
        "to the class label can improve the model's robustness to this type of error. Although, it can be easy to derail the learning process. Adding noise to a continuous target variable in the case of regression or time series forecasting is much like the addition of noise to the input variables and\n",
        "maybe a better use case.\n",
        "\n",
        "> Noise can be added to training regardless of the type of problem that is being addressed. It is appropriate to try adding noise to both classification and regression type problems. The type of noise can be specialized to the types of data used as input to the model, for example,\n",
        "**two-dimensional noise** in the case of **images** and **signal noise** in the case of **audio data**.\n",
        "\n",
        "<font color='red'>Noise is only added during the training of your model</font>. Be sure that any source of noise is not added during the evaluation of your model, or when your model is used to make predictions on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q6tp_fOrVC0"
      },
      "source": [
        "Keras supports the addition of noise to models via the **GaussianNoise layer**. This is a layer that will add noise to inputs of a given shape. The noise has a mean of zero and requires that a standard deviation of the noise be specifed as a hyperparameter. For example:\n",
        "\n",
        "```python\n",
        "# import noise layer\n",
        "from keras.layers import GaussianNoise\n",
        "# define noise layer\n",
        "layer = GaussianNoise(0.1)\n",
        "```\n",
        "\n",
        "The **GaussianNoise layer** can be used in a few different ways with a neural network model. Firstly, it can be used as an input layer to add noise to input variables directly. This is the traditional use of noise as a regularization method in neural networks. Below is an example of\n",
        "defining a **GaussianNoise layer** as an input layer for a model that takes 2 input variables.\n",
        "\n",
        "```python\n",
        "...\n",
        "model.add(GaussianNoise(0.01, input_shape=(2,)))\n",
        "...\n",
        "```\n",
        "\n",
        "**Noise** can also be added **between hidden layers** in the model. Given the flexibility of Keras, the noise can be added before or after the use of the activation function. It may make more sense\n",
        "to add it before the activation; nevertheless, both options are possible. Below is an example of a **GaussianNoise layer** that adds noise to the linear output of a Dense layer before a rectified\n",
        "linear activation function, perhaps a more appropriate use of noise between hidden layers.\n",
        "\n",
        "```python\n",
        "...\n",
        "model.add(Dense(32))\n",
        "model.add(GaussianNoise(0.1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(32))\n",
        "...\n",
        "```\n",
        "\n",
        "Noise can also be added after the activation function, much like using a noisy activation function. One downside of this usage is that the resulting values may be out-of-range from what the activation function may normally provide. For example, a value with added noise may be less than zero, whereas the relu activation function will only ever output values 0 or larger.\n",
        "\n",
        "```python\n",
        "...\n",
        "model.add(Dense(32, activation='reu'))\n",
        "model.add(GaussianNoise(0.1))\n",
        "model.add(Dense(32))\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tBq5rIRngDf"
      },
      "source": [
        "### 2.5.2 Noise Regularization Case Study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-cXewmVq3Kh"
      },
      "source": [
        "# scatter plot of circles dataset\n",
        "from sklearn.datasets import make_circles\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n",
        "\n",
        "# scatter plot for each class value\n",
        "for class_value in range(2):\n",
        "\t# select indices of points with the class label\n",
        "\trow_ix = np.where(y == class_value)\n",
        "\t# scatter plot for points with a different color\n",
        "\tplt.scatter(x[row_ix, 0], x[row_ix, 1])\n",
        "# show plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAC6Y3OrE4s"
      },
      "source": [
        "#### 2.5.2.1 Overfit multilayer perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdqpH6-FsulY"
      },
      "source": [
        "# mlp overfit on the two circles dataset\n",
        "from sklearn.datasets import make_circles\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[: n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMPoBUAOtPap"
      },
      "source": [
        "#### 2.5.2.2 MLP with input layer noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzUITD3hvMJ2"
      },
      "source": [
        "The dataset is defined by points that have a controlled amount of statistical noise. Nevertheless, because the dataset is small, we may wish to add further noise to the input values. This will have the effect of creating more samples or resampling the domain, making the structure of the input space arti\fcially smoother. This may make the problem easier to learn and improve\n",
        "generalization performance. We can add a **GaussianNoise layer** as the **input layer**. The amount of noise must be small. Given that the input values are within the range $[0, 1]$, we will add Gaussian noise with a mean of 0.0 and a standard deviation of 0.1, chosen arbitrarily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiZIvET7t8Q2"
      },
      "source": [
        "# mlp overfit on the two circles dataset with input noise\n",
        "from sklearn.datasets import make_circles\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GaussianNoise\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(GaussianNoise(0.1, input_shape=(2,)))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KOulgIdu7Al"
      },
      "source": [
        "We clearly see the impact of the added noise on the evaluation of the model during training as graphed on the line plot. The noise causes the accuracy of the model to jump around during training, possibly due to the noise introducing points that conflict with true points from the training dataset. Perhaps a lower input noise standard deviation would be more appropriate.\n",
        "\n",
        "The model still shows a pattern of being overfit, with a rise and then fall in test accuracy over training epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrrxGHecvzN7"
      },
      "source": [
        "#### 2.5.3.3 MLP with hidden layer noise\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FxeAHhzwPuC"
      },
      "source": [
        "# mlp overfit on the two circles dataset with hidden layer noise\n",
        "from sklearn.datasets import make_circles\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import GaussianNoise\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2))\n",
        "model.add(GaussianNoise(0.1))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmP9BuA1wyZa"
      },
      "source": [
        "We can also see from the line plot of accuracy over training epochs that the model no longer appears to show the properties of being overfit with regard to classification accuracy. The learning curves for loss do still show a pattern of being overfit.\n",
        "\n",
        "\n",
        "> We can also experiment and add the noise after the outputs of the \frst hidden layer pass through the activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cKZT8M9x79L"
      },
      "source": [
        "# mlp overfit on the two circles dataset with hidden layer noise\n",
        "from sklearn.datasets import make_circles\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import GaussianNoise\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2,activation='relu'))\n",
        "model.add(GaussianNoise(0.1))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdIIqLI0yceb"
      },
      "source": [
        "Surprisingly, we see little difference in the performance of the model, perhaps a small lift in performance.\n",
        "\n",
        "Again, we can see from the line plot of accuracy over training epochs that the model no longer shows sign of overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LdLnu7Uzb0X"
      },
      "source": [
        "## 2.6 Halt Training at the Right Time with Early Stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7paFJWE60yXY"
      },
      "source": [
        "A major challenge in training neural networks is how long to train them. Too little training will mean that the model will underfit the train and the test sets. Too much training will mean that the model will overfit the training dataset and have poor performance on the test set. A compromise is to train on the training dataset but to stop training at the point when performance on a validation dataset starts to degrade. \n",
        "\n",
        "This simple, effective, and widely used approach to training neural networks is called **early stopping**. This section will discover\n",
        "that stopping the training of a neural network early before it has overfited the training dataset can reduce overfitting and improve the generalization of deep neural networks. After reading this section, you will know:\n",
        "\n",
        "- The challenge of training a neural network long enough to learn the mapping, but not so long that it overfits the training data.\n",
        "- Model performance on a holdout validation dataset can be monitored during training and training stopped when generalization error starts to increase.\n",
        "- The use of early stopping requires the selection of a performance measure to monitor, a trigger to stop training, and a selection of the model weights to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwD4awF938vv"
      },
      "source": [
        "### 2.6.1 Early Stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cD7mIh64Nl4"
      },
      "source": [
        "In this section discover the problem of training a model for too long and the regularizing effect that halting the training process at the right time can have, as well as tips for using early\n",
        "stopping in your own projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYRiSpVn4VHv"
      },
      "source": [
        "#### 2.6.1.1 The problem of training just enough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lR03gU_4bBC"
      },
      "source": [
        "Training neural networks is challenging. \n",
        "> When training a large network, there will be a point during training when the model will stop generalizing and start learning the training dataset's statistical noise. \n",
        "\n",
        "This overtting of the training dataset will increase generalization error, making the model less useful in making predictions on new data. \n",
        "\n",
        "**The challenge is to train the network long enough to learn the mapping** from inputs to outputs, **but not training the model so long that it overfits** the training data.\n",
        "\n",
        "One approach to solving this problem is to treat the **number of training epochs as a hyperparameter**, train the model multiple times with different values, and then select the number of epochs that result in the train's best performance or a holdout test dataset. The downside of this approach is that it requires multiple models to be trained and discarded. \n",
        "This can be computationally ineffcient and time-consuming, especially for large models trained on large datasets over days or weeks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyXQA2SB5YQC"
      },
      "source": [
        "#### 2.6.1.2 Stop Training When Generalization Error Increases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUGjr_AT5wOB"
      },
      "source": [
        "An alternative approach is to train the model once for a large number of training epochs. During training, the model is evaluated on a holdout validation dataset after each epoch. If the performance of the model on the validation dataset starts to degrade (e.g. loss begins to\n",
        "increase or accuracy begins to decrease), then the training process is stopped.\n",
        "\n",
        "When training is stopped, the model is then used and is known to have good generalization performance. This procedure is called **early stopping** and is perhaps one of the **oldest and most widely used forms of neural network regularization**.\n",
        "\n",
        "If regularization methods like weight decay that update the loss function to encourage less complex models are considered explicit regularization, then early stopping may be thought of as a type of implicit regularization, much like using a smaller network that has less capacity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrIW72OR6OMD"
      },
      "source": [
        "#### 2.6.1.3 How to stop training early"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My89oH2i6ojK"
      },
      "source": [
        "**Early stopping** requires that you configure your network to be under constrained, **meaning that it has more capacity than is required for the problem**. \n",
        "\n",
        "When training the network, a larger\n",
        "number of training epochs is used than may normally be required, to give the network plenty of opportunity to fit, then begin to overfit the training dataset. There are three elements to using early stopping; they are:\n",
        "\n",
        "- Monitoring model performance.\n",
        "- Trigger to stop training.\n",
        "- The choice of model to use.\n",
        "\n",
        "**Monitoring performance**\n",
        "\n",
        "The performance of the model must be monitored during training. This requires choosing a dataset used to evaluate the model and a metric used to evaluate the model. It is common to split the training dataset and use a subset, such as 30%, as a validation dataset used to monitor the model's performance during training. This validation set is not used to train the model. It is also common to use the loss on a validation dataset as the metric to monitor, although you may also use prediction error in the case of regression, or accuracy in \n",
        "classication.\n",
        "\n",
        "The loss of the model on the training dataset will also be available as part of the training procedure, and additional metrics may also be calculated and monitored on the training dataset. Performance of the model is evaluated on the validation set at the end of each epoch, which\n",
        "adds computational cost during training. This can be reduced by evaluating the model less frequently, such as every 2, 5, or 10 training epochs.\n",
        "\n",
        "**Early Stopping Trigger**\n",
        "\n",
        "Once a scheme for evaluating the model is selected, a trigger for stopping the training process must be chosen. The trigger will use a monitored performance metric to decide when to stop training. This is often the performance of the model on the holdout dataset, such as the loss. \n",
        "\n",
        "In the simplest case, training is stopped as soon as the performance on the validation dataset decreases compared to the performance on the validation dataset at the prior training epoch (e.g. an increase in loss). More elaborate triggers may be required in practice. This is because the training of a neural network is stochastic and can be noisy. Plotted on a graph, a model's performance on a validation dataset may go up and down many times. This means that the first sign of overtting may not be a good place to stop training.\n",
        "\n",
        "**Some more elaborate triggers may include**:\n",
        "\n",
        "- No change in metric over a given number of epochs.\n",
        "- An absolute change in a metric.\n",
        "- A decrease in performance observed over a given number of epochs.\n",
        "- Average change in metric over a given number of epochs.\n",
        "\n",
        "Some delay or patience in stopping is almost always a good idea.\n",
        "\n",
        "\n",
        "**Model Choice**\n",
        "\n",
        "When training is halted, the model is known to have a slightly worse generalization error than a prior epoch model. As such, some consideration may need to be given as to exactly which model is saved. \n",
        "\n",
        "Specifically, the training epoch from which weights in the model\n",
        "are saved to file. This will depend on the trigger chosen to stop the training process. For example:\n",
        "\n",
        "- If the trigger is a simple decrease in performance from one epoch to the next, then the weights for the model at the prior epoch will be preferred. \n",
        "- If the trigger is required to observe a decrease in performance over a fixed number of epochs, then the model at the beginning of the trigger period will be preferred. \n",
        "\n",
        "> **Perhaps a simple approach** is to ,<font color='red'>always save the model weights if the model's performance on a holdout dataset is better than at the previous epoch</font>. That way, you will always have the model with the best performance on the holdout set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9VoJruX8ekC"
      },
      "source": [
        "### 2.6.2 Early Stopping Case Study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4CFYWrgGn3T"
      },
      "source": [
        "# scatter plot of moons dataset\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "# scatter plot for each class value\n",
        "\n",
        "for class_value in range(2):\n",
        "\t# select indices of points with the class label\n",
        "\trow_ix = np.where(y == class_value)\n",
        "\t# scatter plot for points with a different color\n",
        "\tplt.scatter(x[row_ix, 0], x[row_ix, 1])\n",
        "# show plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TQUQB5DG6OM"
      },
      "source": [
        "#### 2.6.2.1 Overfit Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfBbaalKHCV7"
      },
      "source": [
        "# mlp overfit on the moons dataset\n",
        "from sklearn.datasets import make_moons\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqKQznnOHhwu"
      },
      "source": [
        "#### 2.6.2.2 Overfit MLP with early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtGvXwyfH8Xq"
      },
      "source": [
        "# mlp overfit on the moons dataset with simple early stopping\n",
        "from sklearn.datasets import make_moons\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# simple early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[es,early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a85ywHUcItgA"
      },
      "source": [
        "We can also see that the callback stopped training at epoch 239. This is too early as we would expect an early stop to be around epoch 800. This is also highlighted by the classification accuracy on both the train and test sets, which is worse than no early stopping.\n",
        "\n",
        "Reviewing the line plot of train and test loss, we can indeed see that training was stopped at the point when validation loss began to plateau for the \ffirst time.\n",
        "\n",
        "We can improve the trigger for early stopping by waiting a while before stopping. This can be achieved by setting the **patience** argument. In this case, we will wait 200 epochs before training is stopped. Specifically, this means that we will allow training to continue for up to an\n",
        "additional 200 epochs after the point that validation loss started to degrade, giving the training process an opportunity to get across  at spots or find some additional improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avCBvv0IJH9M"
      },
      "source": [
        "# mlp overfit on the moons dataset with patient early stopping\n",
        "from sklearn.datasets import make_moons\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# patient early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[es,early])\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "plt.subplot(211)\n",
        "plt.title('Cross-Entropy Loss', pad=-40)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "plt.subplot(212)\n",
        "plt.title('Accuracy', pad=-40)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I5WEzgOKDwe"
      },
      "source": [
        "Running the example, we can see that training was stopped much later, in this case just before epoch 1,000. We can also see that the performance on the test dataset is better than not using any early\n",
        "stopping. Reviewing the line plot of loss during training, we can see that the patience allowed the training to progress past some small at and bad spots.\n",
        "\n",
        "We can also see that test loss started to increase again in the last approximately 100 epochs. This means that although the model's performance has improved, we may not have the\n",
        "best performing or most stable model at the end of training. We can address this by using a **ModelChecckpoint callback**. In this case, we are interested in saving the model with the best accuracy on the test dataset. We could also seek the model with the best loss on the test\n",
        "dataset, but this may or may not correspond to the best accuracy model.\n",
        "\n",
        "This highlights an important concept in model selection. The notion of the best model during training may conflict when evaluated using different performance measures. \n",
        "\n",
        "> Try to choose models based on the metric by which they will be evaluated and presented in the domain. \n",
        "\n",
        "In a balanced binary classication problem, this will most likely be classification accuracy. Therefore, we will use accuracy on the validation in the **ModelCheckpoint callback** to save the best model observed during training.\n",
        "\n",
        "```python\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1,\n",
        "save_best_only=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBX3R0XIKabv"
      },
      "source": [
        "# mlp overfit on the moons dataset with patient early stopping and model checkpointing\n",
        "from sklearn.datasets import make_moons\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# generate 2d classification dataset\n",
        "x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 30\n",
        "train_x, test_x = x[:n_train, :], x[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# simple early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy',\n",
        "                     mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_x, train_y,\n",
        "                    validation_data=(test_x, test_y),\n",
        "                    epochs=4000, verbose=0, callbacks=[es, mc,early])\n",
        "\n",
        "# load the saved model\n",
        "saved_model = load_model('best_model.h5')\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = saved_model.evaluate(train_x, train_y, verbose=0)\n",
        "_, test_acc = saved_model.evaluate(test_x, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7uZu4C7Lnnl"
      },
      "source": [
        "Running the example, we can see the verbose output from the ModelCheckpoint callback\n",
        "for both when a new best model is saved and from when no improvement was observed. We can see that the best model was observed at epoch 956 during this run.\n",
        "\n",
        "Again, we can see that early stopping continued patiently until after epoch 1,105. Note that\n",
        "epoch 956 + a patience of 200 is not epoch 1,105. \n",
        "\n",
        "> Recall that early stopping is monitoring loss on the validation dataset and that the model checkpoint is saving models based on accuracy. As such, the patience of early stopping started at an epoch other than 956.\n",
        "\n",
        "In this case, we don't see any further improvement in model accuracy on the test dataset. Nevertheless, **we have followed a good practice**. \n",
        "\n",
        "> <font color='red'> Why not monitor validation accuracy for early\n",
        "stopping?</font>\n",
        "\n",
        "**This is a good question**. The main reason is that accuracy is a coarse measure of model performance during training and that **loss provides more nuance** when using early\n",
        "stopping with classification problems. The same measure may be used for early stopping and model checkpointing in the case of regression, such as mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_jqNCkNOD5A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}